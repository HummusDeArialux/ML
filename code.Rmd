---
title: "Predicción de la estructura secundaria de proteínas globulares"
author: "Maria Lucas"
date: "2023-03-24"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
---

\newpage

# Algoritmo k-NN

El algoritmo k-nn (k-nearest neighbors) es un algoritmo de aprendizaje supervisado utilizado para clasificación y regresión. En el proceso de clasificación, el algoritmo busca encontrar la clase más común entre los k ejemplos de entrenamiento más cercanos al punto de consulta. En el proceso de regresión, el algoritmo busca encontrar el valor medio de los k ejemplos de entrenamiento más cercanos al punto de consulta.

El funcionamiento del algoritmo k-nn es bastante sencillo. Primero, se carga un conjunto de datos de entrenamiento que consta de entradas y etiquetas correspondientes. Luego, se toma un punto de consulta (una entrada sin etiquetar) y se calcula la distancia entre ese punto y cada punto en el conjunto de datos de entrenamiento. Las distancias más comunes utilizadas en k-nn son la distancia euclidiana y la distancia Manhattan.

Una vez que se han calculado las distancias, se seleccionan los k puntos de entrenamiento más cercanos al punto de consulta. Si se está realizando clasificación, se seleccionan las etiquetas correspondientes a estos puntos de entrenamiento y se toma la etiqueta más común como la etiqueta asignada al punto de consulta. Si se está realizando regresión, se toma el valor medio de las etiquetas de los k puntos de entrenamiento más cercanos como el valor asignado al punto de consulta. 

La elección del valor de k en el algoritmo k-nn es un factor crítico que puede afectar significativamente el rendimiento del modelo. Si k es demasiado pequeño, el modelo puede ser sensible al ruido en los datos y puede sobreajustarse. Si k es demasiado grande, el modelo puede subajustarse y no ser capaz de capturar patrones sutiles en los datos. El valor de k dependerá del conjunto de datos y el problema específico que se está abordando, aunque se puede empezar por la raíz cuadrada del número de datos e ir ajustando. 

| Ventajas | Inconvenientes |
| ----- | ------- |
| Simple y fácil de interpretar | No produce un modelo |
| Rápida fase de entrenamiento | Lenta fase de clasificación |
| No paramétrico | Se debe escoger una k apropiada |
| Buen rendimiento en datos con pocos atributos | Computacionalmente costoso para gran cantidad de datos |
| Se puede actualizar a tiempo real con nuevos datos | Sensible a datos redundantes y a valores atípicos |
|  | Requiere pre-procesamiento de los datos |

# Codificación one-hot

```{r}
library(reticulate)
use_python("C:/Users/Arialux/AppData/Local/Programs/Python/Python311")
```

```{python}

def encode_sequence(sequence):
    
    # Define a dictionary that maps amino acids to their corresponding positions in the one-hot encoding
    aa_to_index = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}

    # Initialize an empty list to store the encoding
    encoding = []

    # Iterate over each amino acid in the sequence and encode it using the aa_to_index dictionary
    
    for aa in sequence:
        index = aa_to_index[aa]
        encoding.extend([1 if i == index else 0 for i in range(20)])

    return encoding

```

# Clasificador knn

## (a) Carga del fichero y tabla resumen

```{python}

# Carga de los datos
#pip install pandas
import pandas as pd
data = pd.read_csv('data4.csv', delimiter=";").values

```

```{python}

# Creación tabla resumen
coil = 0
bsheet = 0
ahelix = 0

for seq in data:
  if seq[-1] == "_":
    coil += 1
  if seq[-1] == "e":
    bsheet += 1
  if seq[-1] == "h":
    ahelix += 1

#pip install tabulate
from tabulate import tabulate

tabla = [["a-helix", ahelix], ["b-sheet", bsheet], ["coil", coil]]
print(tabulate(tabla, headers=["Estructura", "Contador"]))

```

```{r}
library(reticulate)
py_install("numpy")

py_config("C:/Program Files/Python310")
use_python("C:/Program Files/Python310")
py_module_available("numpy")
```

## Aplicación one-hot




